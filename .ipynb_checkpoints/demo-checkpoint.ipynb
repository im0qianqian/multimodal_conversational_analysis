{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from dataset import MELDDataset, Utterance, Dialogue\n",
    "import pickle\n",
    "from dummy_model import DummyModel\n",
    "from torch.utils.data import DataLoader\n",
    "from models.config import Config\n",
    "from models.dialogue_gcn import DialogueGCN\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joy': 0, 'anger': 1, 'disgust': 2, 'fear': 3, 'sadness': 4, 'neutral': 5, 'surprise': 6}\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "audio_embed_path_test = \"../MELD.Raw/audio_embeddings_feature_selection_emotion.pkl\"\n",
    "_, _, test_audio_emb = pickle.load(open(audio_embed_path_test, 'rb'))\n",
    "test_dataset = MELDDataset(\"../MELD.Raw/test_sent_emo.csv\", \"../MELD.Raw/output_repeated_splits_test\", test_audio_emb, name=\"test\", config=config)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DialogueGCN(\n",
       "  (text_encoder): GRU(768, 100, batch_first=True, bidirectional=True)\n",
       "  (context_encoder): GRU(200, 100, batch_first=True, bidirectional=True)\n",
       "  (pred_rel_l1): GraphConvolution()\n",
       "  (suc_rel_l1): GraphConvolution()\n",
       "  (same_speak_rel_l1): GraphConvolution()\n",
       "  (diff_speak_rel_l1): GraphConvolution()\n",
       "  (pred_rel_l2): GraphConvolution()\n",
       "  (suc_rel_l2): GraphConvolution()\n",
       "  (same_speak_rel_l2): GraphConvolution()\n",
       "  (diff_speak_rel_l2): GraphConvolution()\n",
       "  (edge_att_weights): Linear(in_features=200, out_features=200, bias=False)\n",
       "  (text_attn): Linear(in_features=200, out_features=1, bias=True)\n",
       "  (w_sentiment): Linear(in_features=400, out_features=3, bias=True)\n",
       "  (w_emotion_1): Linear(in_features=400, out_features=200, bias=True)\n",
       "  (w_emotion_2): Linear(in_features=200, out_features=7, bias=True)\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"model_saves/text0_epoch_image_only_7.pt\"\n",
    "model = DialogueGCN(config)\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "state_dict = torch.load(model_path)\n",
    "model.load_state_dict(state_dict['model_state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend = {0:'joy', 1:'anger', 2:'disgust', 3:'fear', 4:'sadness', 5:'neutral', 6:'surprise'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(predicted_emotion, predicted_sentiment, target):\n",
    "    print(target.size())\n",
    "    print(predicted_emotion.size())\n",
    "    print(predicted_sentiment.size())\n",
    "    emotion_accuracy_acc = torch.eq(predicted_emotion, target[:,0]).sum().item() / target.size(0)\n",
    "    sentiment_accuracy_acc = torch.eq(predicted_sentiment, target[:,1]).sum().item() / target.size(0)\n",
    "    return emotion_accuracy_acc, sentiment_accuracy_acc\n",
    "\n",
    "\n",
    "def test_model(model_name, model, test_loader):\n",
    "    print(\"Testing \" + model_name)\n",
    "    model = model.eval()\n",
    "    emotion_predicted_labels = []\n",
    "    sentiment_predicted_labels = []\n",
    "    emotion_target_labels = []\n",
    "    sentiment_target_labels = []\n",
    "    for i, (test_batch_input, test_batch_labels) in enumerate(test_loader):\n",
    "        batch_emotion_correct_predicted_labels, batch_sentiment_predicted_labels, batch_val_count = validate_step(model, test_batch_input, test_batch_labels)\n",
    "        emotion_predicted_labels.append(batch_emotion_correct_predicted_labels)\n",
    "        #print(batch_emotion_correct_predicted_labels)\n",
    "        sentiment_predicted_labels.append(batch_sentiment_predicted_labels)\n",
    "        emotion_target_labels.append(torch.cat(test_batch_labels[0],0))\n",
    "        sentiment_target_labels.append(torch.cat(test_batch_labels[1],0))\n",
    "\n",
    "    emotion_predicted_labels = torch.cat(emotion_predicted_labels, 0).cuda()\n",
    "    sentiment_predicted_labels = torch.cat(sentiment_predicted_labels, 0).cuda()\n",
    "    emotion_target_labels = torch.cat(emotion_target_labels, 0)\n",
    "    sentiment_target_labels = torch.cat(sentiment_target_labels, 0)\n",
    "    target_labels = torch.cat([emotion_target_labels.unsqueeze(1), sentiment_target_labels.unsqueeze(1)], 1).cuda()\n",
    "\n",
    "    emotion_f1_score = f1_score(emotion_target_labels.cpu().numpy(), emotion_predicted_labels.cpu().numpy(), average='weighted')        \n",
    "    confusion = confusion_matrix(emotion_target_labels.cpu().numpy(), emotion_predicted_labels.cpu().numpy())\n",
    "    emotion_accuracy, sentiment_accuracy = get_accuracy(emotion_predicted_labels, sentiment_predicted_labels, target_labels)\n",
    "    #emotion_recalls, sentiment_recalls = get_recall_for_each_class(emotion_predicted_labels, sentiment_predicted_labels, target_labels)\n",
    "    #emotion_precisions, sentiment_precisions = get_precision_for_each_class(emotion_predicted_labels, sentiment_predicted_labels, target_labels)\n",
    "    #emotion_f1s, sentiment_f1s = get_f1_score_for_each_class(emotion_precisions, emotion_recalls, sentiment_precisions, sentiment_recalls)\n",
    "    #emotion_weighted_f1, sentiment_weighted_f1 = get_weighted_F1(emotion_f1s, sentiment_f1s, target_labels)\n",
    "\n",
    "    print(\"Validation Accuracy (Emotion): \", emotion_accuracy)\n",
    "    print(\"F1 Weighted\", emotion_f1_score)\n",
    "    print(\"Confusion matrix\", confusion)\n",
    "\n",
    "def validate_step(model, input, target):\n",
    "    target = torch.LongTensor(target).to(\"cuda\")\n",
    "    (output_logits_emotion, output_logits_sentiment) = model(input)\n",
    "    output_labels_emotion = torch.argmax(output_logits_emotion, dim=1)\n",
    "    output_labels_sentiment = torch.argmax(output_logits_sentiment, dim=1)\n",
    "    return output_labels_emotion, output_labels_sentiment, target[0].size()\n",
    "\n",
    "def preprocess_datapoint(datapoint):\n",
    "    datapoint = list(datapoint[0])\n",
    "    text = datapoint[0]\n",
    "    for i, utt in enumerate(text):\n",
    "        text[i] = (utt,)\n",
    "    datapoint[0] = text    \n",
    "    return datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing GCN_TEXT model\n",
      "Loading data for  0\n",
      "Loading data for  1\n",
      "Loading data for  2\n",
      "Loading data for  3\n",
      "Loading data for  4\n",
      "Loading data for  5\n",
      "Loading data for  6\n",
      "Loading data for  7\n",
      "Loading data for  8\n",
      "Loading data for  9\n",
      "Loading data for  10\n",
      "Loading data for  11\n",
      "Loading data for  12\n",
      "Loading data for  13\n",
      "Loading data for  14\n",
      "Loading data for  15\n",
      "Loading data for  16\n",
      "Loading data for  17\n",
      "Loading data for  18\n",
      "Loading data for  19\n",
      "Loading data for  20\n",
      "Loading data for  21\n",
      "Loading data for  22\n",
      "Loading data for  23\n",
      "Loading data for  24\n",
      "Loading data for  25\n",
      "Loading data for  26\n",
      "Loading data for  27\n",
      "Loading data for  28\n",
      "Loading data for  29\n",
      "Loading data for  30\n",
      "Loading data for  31\n",
      "Loading data for  32\n",
      "Loading data for  33\n",
      "Loading data for  34\n",
      "Loading data for  35\n",
      "Loading data for  36\n",
      "Loading data for  37\n",
      "Loading data for  38\n",
      "Loading data for  39\n",
      "Loading data for  40\n",
      "Loading data for  41\n",
      "Loading data for  42\n",
      "Loading data for  43\n",
      "Loading data for  44\n",
      "Loading data for  45\n",
      "Loading data for  46\n",
      "Loading data for  47\n",
      "Loading data for  48\n",
      "Loading data for  49\n",
      "Loading data for  50\n",
      "Loading data for  51\n",
      "Loading data for  52\n",
      "Loading data for  53\n",
      "Loading data for  54\n",
      "Loading data for  55\n",
      "Loading data for  56\n",
      "Loading data for  57\n",
      "Loading data for  58\n",
      "Loading data for  59\n",
      "Loading data for  60\n",
      "Loading data for  61\n",
      "Loading data for  62\n",
      "Loading data for  63\n",
      "Loading data for  64\n",
      "Loading data for  65\n",
      "Loading data for  66\n",
      "Loading data for  67\n",
      "Loading data for  68\n",
      "Loading data for  69\n",
      "Loading data for  70\n",
      "Loading data for  71\n",
      "Loading data for  72\n",
      "Loading data for  73\n",
      "Loading data for  74\n",
      "Loading data for  75\n",
      "Loading data for  76\n",
      "Loading data for  77\n",
      "Loading data for  78\n",
      "Loading data for  79\n",
      "Loading data for  80\n",
      "Loading data for  81\n",
      "Loading data for  82\n",
      "Loading data for  83\n",
      "Loading data for  84\n",
      "Loading data for  85\n",
      "Loading data for  86\n",
      "Loading data for  87\n",
      "Loading data for  88\n",
      "Loading data for  89\n",
      "Loading data for  90\n",
      "Loading data for  91\n",
      "Loading data for  92\n",
      "Loading data for  93\n",
      "Loading data for  94\n",
      "Loading data for  95\n",
      "Loading data for  96\n",
      "Loading data for  97\n",
      "Loading data for  98\n",
      "Loading data for  99\n",
      "Loading data for  100\n",
      "Loading data for  101\n",
      "Loading data for  102\n",
      "Loading data for  103\n",
      "Loading data for  104\n",
      "Loading data for  105\n",
      "Loading data for  106\n",
      "Loading data for  107\n",
      "Loading data for  108\n",
      "Loading data for  109\n",
      "Loading data for  110\n",
      "Loading data for  111\n",
      "Loading data for  112\n",
      "Loading data for  113\n",
      "Loading data for  114\n",
      "Loading data for  115\n",
      "Loading data for  116\n",
      "Loading data for  117\n",
      "Loading data for  118\n",
      "Loading data for  119\n",
      "Loading data for  120\n",
      "Loading data for  121\n",
      "Loading data for  122\n",
      "Loading data for  123\n",
      "Loading data for  124\n",
      "Loading data for  125\n",
      "Loading data for  126\n",
      "Loading data for  127\n",
      "Loading data for  128\n",
      "Loading data for  129\n",
      "Loading data for  130\n",
      "Loading data for  131\n",
      "Loading data for  132\n",
      "Loading data for  133\n",
      "Loading data for  134\n",
      "Loading data for  135\n",
      "Loading data for  136\n",
      "Loading data for  137\n",
      "Loading data for  138\n",
      "Loading data for  139\n",
      "Loading data for  140\n",
      "Loading data for  141\n",
      "Loading data for  142\n",
      "Loading data for  143\n",
      "Loading data for  144\n",
      "Loading data for  145\n",
      "Loading data for  146\n",
      "Loading data for  147\n",
      "Loading data for  148\n",
      "Loading data for  149\n",
      "Loading data for  150\n",
      "Loading data for  151\n",
      "Loading data for  152\n",
      "Loading data for  153\n",
      "Loading data for  154\n",
      "Loading data for  155\n",
      "Loading data for  156\n",
      "Loading data for  157\n",
      "Loading data for  158\n",
      "Loading data for  159\n",
      "Loading data for  160\n",
      "Loading data for  161\n",
      "Loading data for  162\n",
      "Loading data for  163\n",
      "Loading data for  164\n",
      "Loading data for  165\n",
      "Loading data for  166\n",
      "Loading data for  167\n",
      "Loading data for  168\n",
      "Loading data for  169\n",
      "Loading data for  170\n",
      "Loading data for  171\n",
      "Loading data for  172\n",
      "Loading data for  173\n",
      "Loading data for  174\n",
      "Loading data for  175\n",
      "Loading data for  176\n",
      "Loading data for  177\n",
      "Loading data for  178\n",
      "Loading data for  179\n",
      "Loading data for  180\n",
      "Loading data for  181\n",
      "Loading data for  182\n",
      "Loading data for  183\n",
      "Loading data for  184\n",
      "Loading data for  185\n",
      "Loading data for  186\n",
      "Loading data for  187\n",
      "Loading data for  188\n",
      "Loading data for  189\n",
      "Loading data for  190\n",
      "Loading data for  191\n",
      "Loading data for  192\n",
      "Loading data for  193\n",
      "Loading data for  194\n",
      "Loading data for  195\n",
      "Loading data for  196\n",
      "Loading data for  197\n",
      "Loading data for  198\n",
      "Loading data for  199\n",
      "Loading data for  200\n",
      "Loading data for  201\n",
      "Loading data for  202\n",
      "Loading data for  203\n",
      "Loading data for  204\n",
      "Loading data for  205\n",
      "Loading data for  206\n",
      "Loading data for  207\n",
      "Loading data for  208\n",
      "Loading data for  209\n",
      "Loading data for  210\n",
      "Loading data for  211\n",
      "Loading data for  212\n",
      "Loading data for  213\n",
      "Loading data for  214\n",
      "Loading data for  215\n",
      "Loading data for  216\n",
      "Loading data for  217\n",
      "Loading data for  218\n",
      "Loading data for  219\n",
      "Loading data for  220\n",
      "Loading data for  221\n",
      "Loading data for  222\n",
      "Loading data for  223\n",
      "Loading data for  224\n",
      "Loading data for  225\n",
      "Loading data for  226\n",
      "Loading data for  227\n",
      "Loading data for  228\n",
      "Loading data for  229\n",
      "Loading data for  230\n",
      "Loading data for  231\n",
      "Loading data for  232\n",
      "Loading data for  233\n",
      "Loading data for  234\n",
      "Loading data for  235\n",
      "Loading data for  236\n",
      "Loading data for  237\n",
      "Loading data for  238\n",
      "Loading data for  239\n",
      "Loading data for  240\n",
      "Loading data for  241\n",
      "Loading data for  242\n",
      "Loading data for  243\n",
      "Loading data for  244\n",
      "Loading data for  245\n",
      "Loading data for  246\n",
      "Loading data for  247\n",
      "Loading data for  248\n",
      "Loading data for  249\n",
      "Loading data for  250\n",
      "Loading data for  251\n",
      "Loading data for  252\n",
      "Loading data for  253\n",
      "Loading data for  254\n",
      "Loading data for  255\n",
      "Loading data for  256\n",
      "Loading data for  257\n",
      "Loading data for  258\n",
      "Loading data for  259\n",
      "Loading data for  260\n",
      "Loading data for  261\n",
      "Loading data for  262\n",
      "Loading data for  263\n",
      "Loading data for  264\n",
      "Loading data for  265\n",
      "Loading data for  266\n",
      "Loading data for  267\n",
      "Loading data for  268\n",
      "Loading data for  269\n",
      "Loading data for  270\n",
      "Loading data for  271\n",
      "Loading data for  272\n",
      "Loading data for  273\n",
      "Loading data for  274\n",
      "Loading data for  275\n",
      "Loading data for  276\n",
      "Loading data for  277\n",
      "Loading data for  278\n",
      "Loading data for  279\n",
      "torch.Size([2610, 2])\n",
      "torch.Size([2610])\n",
      "torch.Size([2610])\n",
      "Validation Accuracy (Emotion):  0.6030651340996168\n",
      "F1 Weighted 0.5775735176864728\n",
      "Confusion matrix [[ 211   32    1    0    5  107   46]\n",
      " [  38  141    6    1    9  102   48]\n",
      " [   6   14    1    0    5   31   11]\n",
      " [   4    8    0    3    5   22    8]\n",
      " [  21   24    0    1   29  111   22]\n",
      " [  81   25    3    2   24 1022   99]\n",
      " [  34   26    0    0    2   52  167]]\n"
     ]
    }
   ],
   "source": [
    "test_model(\"GCN_TEXT model\", model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for  30\n",
      "Loading data for  30\n",
      "Loading data for  30\n",
      "surprise  :  ['Oh my God, what happened?', 'Oh. God, crazy Chandler. He spun me...off...the...bed!', 'Wow! Spinning that sounds like fun.', \"Oh,  I wish. No, you know he was just trying Ross's Hug and Roll thing.\", \"Ross's what?\", 'You know what, where he hugs you and kinda rolls you away and... Oh... my....God.']\n",
      "Loading data for  30\n",
      "surprise  :  ['', '', '', '', '', '']\n",
      "Loading data for  30\n",
      "joy  :  [array([0.08094506, 0.        , 0.17449932, ..., 0.23509485, 0.54376392,\n",
      "       0.33151705]), array([0.25093461, 0.        , 0.13984529, ..., 0.21948618, 0.48416549,\n",
      "       0.29720939]), array([0.90917832, 0.87294917, 0.26164095, ..., 0.25420537, 0.32039876,\n",
      "       0.38785322]), array([0.30847348, 0.88298553, 0.21380787, ..., 0.29969429, 0.38305547,\n",
      "       0.38591971]), array([0.32422478, 0.        , 0.04765311, ..., 0.16694304, 0.41292769,\n",
      "       0.20914634]), array([0.99871564, 0.39416134, 0.19103805, ..., 0.22618025, 0.47380316,\n",
      "       0.29116776])]\n",
      "Loading data for  30\n",
      "neutral  :  tensor([0, 1, 0, 1, 0, 1])\n",
      "Loading data for  30\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-26619264f88a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdialogue_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "dialogue_id = 30\n",
    "datapoint = preprocess_datapoint(test_dataset[dialogue_id])\n",
    "labels = test_dataset[dialogue_id][1]\n",
    "predictions = validate_step(model, datapoint, labels)[0]\n",
    "\n",
    "for i, pred in enumerate(predictions):\n",
    "    print(legend[int(pred)], \" : \", test_dataset[dialogue_id][0][0][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for  30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Oh my God, what happened?',\n",
       " 'Oh. God, crazy Chandler. He spun me...off...the...bed!',\n",
       " 'Wow! Spinning that sounds like fun.',\n",
       " \"Oh,  I wish. No, you know he was just trying Ross's Hug and Roll thing.\",\n",
       " \"Ross's what?\",\n",
       " 'You know what, where he hugs you and kinda rolls you away and... Oh... my....God.']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[dialogue_id][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [(0, \"maybe she just likes me for me.\"),\n",
    "(1, \"you want these?\"),\n",
    "(2, \"what?\"),\n",
    "(1, \"I stink!\"),\n",
    "(1, \"I can’t play!\"), \n",
    "(1, \"The ball is just sitting there and I can’t hit it!\"),\n",
    "(1, \"I only hit one really good ball that went way out!\"),\n",
    "(1, \"I have no concentration!\"),\n",
    "(2, \"What’s wrong?\"),\n",
    "(1, \"I can’t get rid of the sand.\"),\n",
    "(1, \"There is still some in here won’t go away.\"),\n",
    "(1, \"I even got sand in the pockets.\"),\n",
    "(2, \"Come on, you are getting that all over the floor.\")\n",
    "]\n",
    "utterances = []\n",
    "for i, text_speaker in enumerate(texts):\n",
    "    speaker, text = text_speaker\n",
    "    utterance = Utterance(0, i, text, speaker, 0, 0, 0, 0, 0)\n",
    "    utterances.append(utterance)\n",
    "dialogue = Dialogue(0, utterances, visual_features=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for  0\n"
     ]
    }
   ],
   "source": [
    "datapoint = preprocess_datapoint(dialogue.get_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral  :  maybe she just likes me for me.\n",
      "surprise  :  you want these?\n",
      "surprise  :  what?\n",
      "joy  :  I stink!\n",
      "anger  :  I can’t play!\n",
      "anger  :  The ball is just sitting there and I can’t hit it!\n",
      "anger  :  I only hit one really good ball that went way out!\n",
      "anger  :  I have no concentration!\n",
      "neutral  :  What’s wrong?\n",
      "neutral  :  I can’t get rid of the sand.\n",
      "neutral  :  There is still some in here won’t go away.\n",
      "neutral  :  I even got sand in the pockets.\n",
      "neutral  :  Come on, you are getting that all over the floor.\n"
     ]
    }
   ],
   "source": [
    "for i, pred in enumerate(validate_step(model, datapoint, [0])[0]):\n",
    "    print(legend[int(pred)], \" : \", texts[i][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
